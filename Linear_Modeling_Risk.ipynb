{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riskdata = os.path.join(os.getcwd(), \"RiskCodingChallenge.csv\")\n",
    "df = pd.read_csv(riskdata)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the data is getting cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data\n",
    "columns_to_clean = ['x1', 'x2', 'x3','x4']\n",
    "df[columns_to_clean] = df[columns_to_clean].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "cleaned_file = 'cleaned_data.csv'\n",
    "df.to_csv(cleaned_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering the coefficients by combining my linear algebra and machine learning knowledge to successfully train and fit the 5 chosen models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds a column of ones for the intercept\n",
    "X = np.c_[np.ones(df.shape[0]), df[['x1', 'x2', 'x3', 'x4']].values]\n",
    "\n",
    "#Data column 'y' is the one we want to predict\n",
    "y = df['y'].values\n",
    "\n",
    "#Splitting training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Standardize the features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Initializing all models\n",
    "linear_reg = LinearRegression()\n",
    "decision_tree_reg = DecisionTreeRegressor()\n",
    "random_forest_reg = RandomForestRegressor()\n",
    "gradient_boosting_reg = GradientBoostingRegressor()\n",
    "\n",
    "#Build the neural network model\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "\n",
    "#Training all models\n",
    "linear_reg.fit(X_train,y_train)\n",
    "decision_tree_reg.fit(X_train,y_train)\n",
    "random_forest_reg.fit(X_train,y_train)\n",
    "gradient_boosting_reg.fit(X_train,y_train)\n",
    "mlp_reg.fit(X_train, y_train)\n",
    "\n",
    "#Projecting the coefficients using np.dot() and np.linalg.inv()\n",
    "projection_matrix = np.dot(np.linalg.inv(np.dot(X_train.T, X_train)), X_train.T)\n",
    "coefficients_projected = np.dot(projection_matrix.T, linear_reg.coef_)\n",
    "\n",
    "#Extracting only the coefficients for 'x1', 'x2', 'x3', and 'x4'\n",
    "projected_coefficients = coefficients_projected\n",
    "\n",
    "print(\"Original Coefficients:\", linear_reg.coef_[0:5])\n",
    "\n",
    "print(\"Projected Coefficients:\", projected_coefficients[0:4])\n",
    "\n",
    "#Evaluating all models\n",
    "y_pred_linear = linear_reg.predict(X_test)\n",
    "r_squared_linear = r2_score(y_test, y_pred_linear)\n",
    "print(\"R-squared for linear:\", r_squared_linear)\n",
    "y_pred_decision_tree = decision_tree_reg.predict(X_test)\n",
    "r_squared_decision_tree = r2_score(y_test, y_pred_decision_tree)\n",
    "print(\"R-squared for decision tree:\", r_squared_decision_tree)\n",
    "y_pred_random_forest = random_forest_reg.predict(X_test)\n",
    "r_squared_random_forest = r2_score(y_test, y_pred_random_forest)\n",
    "print(\"R-squared for random forest:\", r_squared_random_forest)\n",
    "y_pred_gradient_boosting = gradient_boosting_reg.predict(X_test)\n",
    "r_squared_gradient_boosting = r2_score(y_test, y_pred_gradient_boosting)\n",
    "print(\"R-squared for gradient boosting:\", r_squared_gradient_boosting)\n",
    "y_pred_mlp = mlp_reg.predict(X_test)\n",
    "r_squared_mlp = r2_score(y_test, y_pred_mlp)\n",
    "print(\"R-squared for MLP:\", r_squared_mlp)\n",
    "\n",
    "#R-squared, also known as the coefficient of determination, represents the proportion of the dependent variable's variance that\n",
    "# is captured by the independent variables in the model. A higher R-squared indicates a better fit, \n",
    "#implying that a larger percentage of the variability in the response variable is accounted for by the model.\n",
    "\n",
    "# Print the predicted y values for each model\n",
    "print(\"Predicted y values for Linear Regression:\", y_pred_linear)\n",
    "print(\"Predicted y values for Decision Tree:\", y_pred_decision_tree)\n",
    "print(\"Predicted y values for Random Forest:\", y_pred_random_forest)\n",
    "print(\"Predicted y values for Gradient Boosting:\", y_pred_gradient_boosting)\n",
    "print(\"Predicted y values for MLP:\", y_pred_mlp)\n",
    "\n",
    "#I can now compare the performance of the different models. \n",
    "#The model that has the highest R squared will be chosen. \n",
    "#The final model will finally be trained and applied to make predictions on new data. \n",
    "best_model = max(r_squared_linear, r_squared_decision_tree, r_squared_random_forest, r_squared_gradient_boosting, r_squared_mlp)\n",
    "print(\"Best Model (highest R-Squared):\", best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing R squared values, the best model was the MLP model. MLP (Multilayer Perceptron) is a type of artificial neural network and is considered a powerful and versatile model for various machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "#Perform hypothesis test using an F-test\n",
    "f_statistic, p_value = f_regression(X[:, 1:], y)\n",
    "\n",
    "#Display results\n",
    "print(\"R Squared:\", best_model)\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "#Check if the p-value is less than the significance level (alpha)\n",
    "alpha = 0.05\n",
    "if p_value[0] < alpha:\n",
    "    print(\"Reject the null hypothesis. The overall model has significant predictive power.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. The overall model may not have significant predictive power.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to reject the null hypothesis proves the \"best model\" we received early has significant predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose Gaussian Process Regressor as my non-linear model because Gaussian processes are non-parametric models, meaning they can capture complex and non-linear relationships in the data. This flexibility is beneficial when dealing with datasets that exhibit intricate patterns or nonlinear dependencies between features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "from scipy.stats import norm, t\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the Gaussian Process kernel\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
    "\n",
    "# Create the Gaussian Process Regressor\n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n",
    "\n",
    "# Fit the Gaussian Process model\n",
    "gp_model.fit(X_train, y_train)\n",
    "\n",
    "# Define negative log-likelihood as a custom scoring function\n",
    "def neg_log_likelihood(estimator, X, y):\n",
    "    y_pred, sigma = estimator.predict(X, return_std=True)\n",
    "    nll = -0.5 * np.sum(np.log(2 * np.pi * sigma**2) + ((y - y_pred) / sigma)**2)\n",
    "    return nll\n",
    "\n",
    "# Evaluate the model using negative log-likelihood\n",
    "nll = cross_val_score(gp_model, X_test, y_test, cv=5, scoring=neg_log_likelihood).mean()\n",
    "print(\"Negative Log-Likelihood:\", nll)\n",
    "\n",
    "# Perform confidence interval to show predictive power for the models using confidence level of Î± = 0.05\n",
    "alpha = 0.05\n",
    "z_value = t.ppf(1 - alpha / 2, len(y_test) - 1)\n",
    "y_pred, sigma = gp_model.predict(X_test, return_std=True)\n",
    "margin_of_error = z_value * sigma\n",
    "critical_value = norm.ppf(1 - alpha / 2)\n",
    "\n",
    "if abs(z_value) > critical_value:\n",
    "    print(f\"The Z-value {z_value} is statistically significant at the {1 - alpha:.0%} confidence level.\")\n",
    "else:\n",
    "    print(f\"The Z-value {z_value} is not statistically significant at the {1 - alpha:.0%} confidence level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "In conclusion, the provided code demonstrates the application of various regression models, including linear regression, decision tree regression, random forest regression, gradient boosting regression, and a multi-layer perceptron (MLP) neural network, to predict a target variable based on a set of input features. The models are trained and evaluated using R-squared. The selection of the best model is based on the highest R squared, which is a key metric for assessing the accuracy of the regression models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
